{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77caeb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic data compilation\n",
    "\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import os\n",
    "import pycountry\n",
    "import re\n",
    "\n",
    "# Read IRENA capacity and generation files\n",
    "df_irena_c = pd.read_excel(\"data/existing_stock/IRENASTAT-C.xlsx\")\n",
    "df_irena_g = pd.read_excel(\"data/existing_stock/IRENASTAT-G.xlsx\")\n",
    "\n",
    "\n",
    "df_ember = pd.read_csv(\"data/existing_stock/yearly_full_release_long_format.csv\")\n",
    "\n",
    "df_gem = pd.read_excel(\"data/existing_stock/Global-Integrated-Power-April-2025.xlsx\", sheet_name=\"Power facilities\")\n",
    "df_gem_map = pd.read_excel(\"assumptions/VS_mappings.xlsx\", sheet_name=\"gem_techmap\")\n",
    "df_irena_ember_map = pd.read_excel(\"assumptions/VS_mappings.xlsx\", sheet_name=\"irena_ember_typemap\")\n",
    "\n",
    "# Remove records where Status starts with 'cancelled', 'shelved', or 'retired'\n",
    "df_gem = df_gem[~df_gem['Status'].str.lower().str.startswith(('cancelled', 'shelved', 'retired'))]\n",
    "\n",
    "# Technology = Type for if missing\n",
    "df_gem['Technology'] = df_gem.apply(\n",
    "    lambda row: row['Type'] if pd.isna(row['Technology']) else row['Technology'], axis=1\n",
    ")\n",
    "\n",
    "# Addin the key field model_fuel to GEM\n",
    "def custom_fuel(row):\n",
    "    if row['Type'] != 'oil/gas':\n",
    "        if row['Type'] == 'hydropower':\n",
    "            return 'hydro'\n",
    "        else:\n",
    "            return row['Type']\n",
    "    else:\n",
    "        if pd.notna(row['Fuel']):\n",
    "            fuel_val = str(row['Fuel'])\n",
    "        else:\n",
    "            fuel_val = ''\n",
    "        if fuel_val.lower().startswith('fossil liquids:'):\n",
    "            return 'oil'\n",
    "        else:\n",
    "            return 'gas'\n",
    "\n",
    "df_gem['model_fuel'] = df_gem.apply(custom_fuel, axis=1)\n",
    "\n",
    "# Clean country names in IRENA dataframes\n",
    "def clean_country_name(country_name):\n",
    "    \"\"\"\n",
    "    Remove trailing information in brackets, e.g., 'Country Name (the)' -> 'Country Name'\n",
    "    \"\"\"\n",
    "    if isinstance(country_name, str):\n",
    "        # Remove any trailing bracketed info, e.g., \"Country (the)\" -> \"Country\"\n",
    "        return re.sub(r'\\s*\\(.*\\)\\s*$', '', country_name).strip()\n",
    "    return country_name\n",
    "\n",
    "\n",
    "df_irena_c['Country/area'] = df_irena_c['Country/area'].apply(clean_country_name)\n",
    "df_irena_g['Country/area'] = df_irena_g['Country/area'].apply(clean_country_name)\n",
    "\n",
    "\n",
    "df_ember = df_ember.rename(columns={'Variable': 'Type', 'Country code': 'iso_code'})\n",
    "df_ember = df_ember[df_ember['Subcategory'] == 'Fuel']\n",
    "\n",
    "#include model_fuel\n",
    "df_ember = duckdb.sql(\"SELECT T2.model_fuel,T1.* FROM df_ember T1 inner join df_irena_ember_map T2 ON T1.Type=T2.Type and T2.Source='EMBER'\").df()\n",
    "\n",
    "# Rename 'Technology' column to 'Type' in IRENA dataframes if present\n",
    "df_irena_c = df_irena_c.rename(columns={'Technology': 'Type'})\n",
    "df_irena_g = df_irena_g.rename(columns={'Technology': 'Type'})\n",
    "\n",
    "# add iso_code to df_irena_c and df_irena_g\n",
    " # Special arrangement to recognize the following:\n",
    "    # Kosovo, Chinese Taipei, Republic of Korea, China, Hong Kong Special Administrative Region, Democratic Republic of the Congo\n",
    "def get_iso_code(country_name):\n",
    "    if not isinstance(country_name, str):\n",
    "        return None\n",
    "    name = country_name.strip().lower()\n",
    "    # Manual mappings for special cases\n",
    "    special_cases = {\n",
    "        'kosovo': 'XKX',\n",
    "        'kosovo (under unscr 1244/99)': 'XKX',\n",
    "        'chinese taipei': 'TWN',\n",
    "        'republic of korea': 'KOR',\n",
    "        'china, hong kong special administrative region': 'HKG',\n",
    "        'democratic republic of the congo': 'COD',\n",
    "        'Russia': 'RUS',\n",
    "        'DR Congo': 'COD',\n",
    "        'Kosovo': 'XKX'\n",
    "    }\n",
    "    if name in special_cases:\n",
    "        return special_cases[name]\n",
    "    try:\n",
    "        return pycountry.countries.lookup(country_name).alpha_3\n",
    "    except (LookupError, AttributeError, ImportError):\n",
    "        return None\n",
    "\n",
    "\n",
    "#Add ISO codes to IRENA and GEM dataframes\n",
    "df_irena_c['iso_code'] = df_irena_c['Country/area'].apply(get_iso_code)\n",
    "df_irena_g['iso_code'] = df_irena_g['Country/area'].apply(get_iso_code)\n",
    "df_gem['iso_code'] = df_gem['Country/area'].apply(get_iso_code)\n",
    "\n",
    "df_irena_c = duckdb.sql(\"SELECT T2.model_fuel,T1.* FROM df_irena_c T1 inner join df_irena_ember_map T2 ON T1.Type=T2.Type and T2.Source='IRENA'\").df()\n",
    "df_irena_g = duckdb.sql(\"SELECT T2.model_fuel,T1.* FROM df_irena_g T1 inner join df_irena_ember_map T2 ON T1.Type=T2.Type and T2.Source='IRENA'\").df()\n",
    "\n",
    "# IAMC data\n",
    "downscaled_file_path = \"data/NGFS4.2/Downscaled_MESSAGEix-GLOBIOM 1.1-M-R12_data.xlsx\"\n",
    "if os.path.exists(downscaled_file_path):\n",
    "    ngfs_df = pd.read_excel(downscaled_file_path)\n",
    "else:\n",
    "    print(f\"File not found: {downscaled_file_path}\")\n",
    "\n",
    "# UNSD\n",
    "\"\"\"\n",
    "sector in ('Power','Industry','Residential','Services','Agriculture','Other','Energy','imports','exports')\n",
    "attribute in ('transformation','consumption','import','export')  \n",
    "\"\"\"\n",
    "\n",
    "df_unsd = pd.read_csv(\"data/unsd/unsd_july_2025.csv\")\n",
    "\n",
    "df_unsd_regmap = pd.read_excel(\"assumptions/VS_mappings.xlsx\", sheet_name=\"unsd_region_map\")\n",
    "df_unsd_prodmap = pd.read_excel(\"assumptions/VS_mappings.xlsx\", sheet_name=\"unsd_product_map\")\n",
    "df_unsd_flowmap = pd.read_excel(\"assumptions/VS_mappings.xlsx\", sheet_name=\"unsd_flow_map\")\n",
    "\n",
    "df_unsd_trade = duckdb.sql(f\"\"\"\n",
    "    SELECT T1.TIME_PERIOD AS year,round(SUM(cast(T1.OBS_VALUE as float) * T1.CONVERSION_FACTOR / 1000 / 3.6),1) as twh_UNSD\n",
    "    ,T3.attribute,T4.ISO\n",
    "    from \n",
    "    df_unsd T1\n",
    "    inner join df_unsd_prodmap T2 ON cast(T2.Code as varchar) = cast(T1.COMMODITY as varchar)\n",
    "    inner join df_unsd_flowmap T3 ON cast(T3.Code as varchar) = cast(T1.TRANSACTION as varchar)\n",
    "    inner join df_unsd_regmap T4 ON cast(T4.Code as varchar) = cast(T1.REF_AREA as varchar)\n",
    "    where \n",
    "    commodity = 7000 AND\n",
    "     T3.attribute.lower() IN ('import','export') AND\n",
    "     T1.TIME_PERIOD >= 2000\n",
    "    group by T1.TIME_PERIOD,T3.attribute,T4.ISO\n",
    "\"\"\").df()\n",
    "\n",
    "\n",
    "df_electricity_trade = df_unsd_trade.pivot(index=[\"ISO\", \"attribute\"], columns=\"year\", values=\"twh_UNSD\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d38e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect information to be able to choose between EMBER and IRENA for utilization factors and efficiency tuning to calibrate to UNSD fuel consumption numbers.\n",
    "def calibration_data(input_iso,df_irena_util,df_ember_util,df_grouped_gem):\n",
    "\n",
    "    import duckdb\n",
    "    import pandas as pd\n",
    "    import xlwings as xw\n",
    "\n",
    "    duckdb.register('df_irena_util', df_irena_util)\n",
    "    duckdb.register('df_ember_util', df_ember_util)\n",
    "    duckdb.register('df_grouped_gem', df_grouped_gem)\n",
    "\n",
    "\n",
    "    result = duckdb.sql(\"\"\"\n",
    "        SELECT model_fuel,sum(cast(Capacity_GW as float)) as Capacity_GW\n",
    "        FROM df_grouped_gem\n",
    "        where \"Start year\" <= '2022'\n",
    "        GROUP BY model_fuel\n",
    "        order by model_fuel\n",
    "    \"\"\").df()\n",
    "\n",
    "    result = duckdb.sql(\"\"\"\n",
    "        SELECT model_fuel,sum(cast(Capacity_GW as float)) as Capacity_GW\n",
    "        FROM df_ember_util\n",
    "        where \"year\" = '2022'\n",
    "        GROUP BY model_fuel\n",
    "        order by model_fuel\n",
    "    \"\"\").df()\n",
    "\n",
    "    result = duckdb.sql(\"\"\"\n",
    "        SELECT model_fuel,sum(cast(Capacity_GW as float)) as Capacity_GW\n",
    "        FROM df_irena_util\n",
    "        where \"year\" = '2022'\n",
    "        GROUP BY model_fuel\n",
    "        order by model_fuel\n",
    "    \"\"\").df()\n",
    "\n",
    "    result = duckdb.sql(\"\"\"\n",
    "        SELECT T1.iso_code, T1.model_fuel,\n",
    "        round(SUM(T1.capacity_gw),1) as capacity_gw_gem,\n",
    "        round(T2.capacity_gw,1) as capacity_gw_irena,\n",
    "        round(T3.capacity_gw,1) as capacity_gw_ember,\n",
    "        \n",
    "        round(SUM(T1.capacity_gw * T2.utilization_factor * 8.76),1) AS generation_twh_gem_irena,\n",
    "        round(SUM(T1.capacity_gw * T3.utilization_factor * 8.76),1) AS generation_twh_gem_ember,\n",
    "        round(T2.generation_twh,1) as generation_twh_irena,round(T3.generation_twh,1) as generation_twh_ember,\n",
    "        \n",
    "        round(T2.utilization_factor,2) as utilization_factor_irena,\n",
    "        round(T3.utilization_factor,2) as utilization_factor_ember,\n",
    "        round(SUM(T1.capacity_gw * coalesce(T1.efficiency, 1)) / SUM(T1.capacity_gw),2) AS avg_efficiency,\n",
    "        round(SUM(T1.capacity_gw * T2.utilization_factor * 8.76 / coalesce(T1.efficiency, 1)),1) AS fuel_consumed_twh_irena,\n",
    "        round(SUM(T1.capacity_gw * T3.utilization_factor * 8.76 / coalesce(T1.efficiency, 1)),1) AS fuel_consumed_twh_ember\n",
    "        \n",
    "        FROM df_grouped_gem T1\n",
    "        LEFT join df_irena_util T2 ON T1.model_fuel = T2.model_fuel AND T2.year = '2022'\n",
    "        LEFT join df_ember_util T3 ON T1.model_fuel = T3.model_fuel AND T3.year = '2022'\n",
    "        where \"Start year\" <= '2022'\n",
    "        GROUP BY T1.iso_code, T1.model_fuel,T2.generation_twh,T3.generation_twh,T2.utilization_factor,T3.utilization_factor,T2.capacity_gw,T3.capacity_gw\n",
    "        order by T1.iso_code, T1.model_fuel\n",
    "    \"\"\").df()\n",
    "\n",
    "\n",
    "\n",
    "    df_unsd_iso = duckdb.sql(f\"\"\"\n",
    "        SELECT T1.TIME_PERIOD AS year,T2.model_fuel,round(SUM(cast(T1.OBS_VALUE as float) * T1.CONVERSION_FACTOR / 1000 / 3.6),1) as fuel_consumed_unsd_twh\n",
    "        from \n",
    "        df_unsd T1\n",
    "        inner join df_unsd_prodmap T2 ON cast(T2.Code as varchar) = cast(T1.COMMODITY as varchar)\n",
    "        inner join df_unsd_flowmap T3 ON cast(T3.Code as varchar) = cast(T1.TRANSACTION as varchar)\n",
    "        where \n",
    "        -- TIME_PERIOD = 2020 AND\n",
    "        REF_AREA IN (select code from df_unsd_regmap where ISO = '{input_iso}')\n",
    "        AND T3.attribute.lower() = 'transformation'\n",
    "        AND T3.sector.lower() = 'power'\n",
    "        group by T1.TIME_PERIOD,T2.model_fuel\n",
    "    \"\"\").df()\n",
    "\n",
    "\n",
    "    # Export the first DataFrame as three separate tables (GW cols, then TWh cols, then remaining cols), one below the other, then the second DataFrame\n",
    "\n",
    "    # Identify GW columns (ending with '_gw'), TWh columns (ending with '_twh'), and the rest\n",
    "    gw_cols = result.columns[:2].tolist() + [col for col in result.columns if '_gw' in col]\n",
    "    twh_cols = result.columns[:2].tolist() + [col for col in result.columns if 'generation_twh' in col]\n",
    "    other_cols = result.columns[:2].tolist() + [col for col in result.columns if col not in gw_cols + twh_cols]\n",
    "    base_year_cols = result.columns[:2].tolist() + [col for col in result.columns if 'generation_twh' in col or 'utilization' in col]\n",
    "\n",
    "    # Prepare the three tables\n",
    "    df_gw = result[gw_cols]\n",
    "    df_twh = result[twh_cols]\n",
    "    df_other = result[other_cols]\n",
    "    df_base_year = result[base_year_cols]\n",
    "    \n",
    "    output_path = f\"output/VerveStacks_{input_iso}.xlsx\"\n",
    "\n",
    "    # Write df_unsd_iso to the right of df_other (i.e., as additional columns)\n",
    "    # First, align df_unsd_iso to have the same number of rows as df_other (if possible)\n",
    "    # We'll join on 'model_fuel' if present in both, else just concat columns\n",
    "\n",
    "\n",
    "    df_unsd_iso = df_unsd_iso[df_unsd_iso['year'] == 2022]\n",
    "    df_unsd_iso = df_unsd_iso.drop(columns=['year'])\n",
    "\n",
    "\n",
    "    # Check if 'model_fuel' is in both DataFrames to allow merge\n",
    "    if 'model_fuel' in df_other.columns and 'model_fuel' in df_unsd_iso.columns:\n",
    "        df_combined = pd.merge(df_other, df_unsd_iso, on='model_fuel', how='left')\n",
    "    else:\n",
    "        # fallback: concat as columns (may misalign if row counts differ)\n",
    "        df_combined = pd.concat([df_other.reset_index(drop=True), df_unsd_iso.reset_index(drop=True)], axis=1)\n",
    "\n",
    "\n",
    "    # Function to delete a sheet if it exists in the Excel file\n",
    "    def delete_sheet_if_exists(file_path, sheet_name):\n",
    "        try:\n",
    "            app = xw.App(visible=False)\n",
    "            wb = app.books.open(file_path)\n",
    "            if sheet_name in [ws.name for ws in wb.sheets]:\n",
    "                wb.sheets[sheet_name].delete()\n",
    "                wb.save()\n",
    "            wb.close()\n",
    "            app.quit()\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not delete sheet '{sheet_name}' from {file_path}: {e}\")\n",
    "\n",
    "    # Example usage: delete the 'Calibration' sheet if it exists before writing\n",
    "    delete_sheet_if_exists(output_path, 'Calibration')\n",
    "\n",
    "\n",
    "    # Using xlwings to write multiple tables to the same sheet\n",
    "    app = xw.App(visible=False)\n",
    "    try:\n",
    "        wb = app.books.open(output_path)\n",
    "        \n",
    "        # Create or get the Calibration sheet\n",
    "        if 'Calibration' in [ws.name for ws in wb.sheets]:\n",
    "            ws_calib = wb.sheets['Calibration']\n",
    "            ws_calib.clear()\n",
    "        else:\n",
    "            ws_calib = wb.sheets.add('Calibration')\n",
    "        \n",
    "        startrow = 1  # xlwings uses 1-based indexing\n",
    "        # Write GW columns table\n",
    "        ws_calib.range(f'A{startrow}').value = [df_gw.columns.tolist()] + df_gw.values.tolist()\n",
    "        startrow += len(df_gw) + 3  # leave a blank row\n",
    "\n",
    "        # Write TWh columns table\n",
    "        ws_calib.range(f'A{startrow}').value = [df_twh.columns.tolist()] + df_twh.values.tolist()\n",
    "        startrow += len(df_twh) + 3  # leave a blank row\n",
    "\n",
    "        # Write other columns table\n",
    "        ws_calib.range(f'A{startrow}').value = [df_combined.columns.tolist()] + df_combined.values.tolist()\n",
    "        \n",
    "        # Create or get the base_year_data sheet\n",
    "        if 'base_year_data' in [ws.name for ws in wb.sheets]:\n",
    "            ws_base = wb.sheets['base_year_data']\n",
    "            ws_base.clear()\n",
    "        else:\n",
    "            ws_base = wb.sheets.add('base_year_data')\n",
    "        \n",
    "        # Write base year data\n",
    "        ws_base.range('A1').value = [df_base_year.columns.tolist()] + df_base_year.values.tolist()\n",
    "        \n",
    "        wb.save()\n",
    "        wb.close()\n",
    "    finally:\n",
    "        app.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d550bba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_iamc_data(input_iso):\n",
    "\n",
    "\n",
    "    # collect data to construct future scenarios using published IAMC results\n",
    "    import matplotlib.pyplot as plt\n",
    "    import math\n",
    "\n",
    "    # Read the Downscaled_MESSAGEix-GLOBIOM 1.1-M-R12_data file and filter on Region='IND'\n",
    "    df_varbl = pd.read_excel(\"assumptions/VS_mappings.xlsx\", sheet_name=\"iamc_variables\")\n",
    "    # Filter for potential_use in 'demand_projection' or 'fuel_supply'\n",
    "    df_varbl = df_varbl[df_varbl['potential_use'].isin(['demand_projection', 'fuel_supply'])]\n",
    "\n",
    "    sheet_name = 'iamc_data'\n",
    "\n",
    "    ngfs_df_iso = ngfs_df[ngfs_df['Region'] == input_iso]\n",
    "    ngfs_df_iso = ngfs_df_iso[ngfs_df_iso['Variable'].isin(df_varbl['variable'])]\n",
    "    ngfs_df_iso = ngfs_df_iso.dropna(axis=1, how='all')\n",
    "\n",
    "\n",
    "    # display(ngfs_df_iso)\n",
    "\n",
    "    # For each variable, plot a line chart with scenario in the legend, sorted by the 'potential_use' column,\n",
    "    # and display the unit as the y-axis label. Display the charts in two columns.\n",
    "\n",
    "\n",
    "    # Merge in the 'potential_use' and 'unit' info for sorting and labeling\n",
    "    df_varbl_for_merge = df_varbl[['variable', 'potential_use','commodity']].drop_duplicates()\n",
    "    ngfs_df_iso_with_pu = ngfs_df_iso.merge(\n",
    "        df_varbl_for_merge,\n",
    "        left_on='Variable',\n",
    "        right_on='variable',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Get unique variables sorted by 'potential_use'\n",
    "    variables_sorted = (\n",
    "        ngfs_df_iso_with_pu[['Variable', 'potential_use', 'Unit']]\n",
    "        .drop_duplicates()\n",
    "        .sort_values(by='potential_use', na_position='last')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    years = [col for col in ngfs_df_iso.columns if str(col).isdigit()]\n",
    "\n",
    "    n_vars = len(variables_sorted)\n",
    "    n_cols = 2\n",
    "    n_rows = math.ceil(n_vars / n_cols)\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 5 * n_rows), squeeze=False)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, row in variables_sorted.iterrows():\n",
    "        var = row['Variable']\n",
    "        unit = row['Unit'] if pd.notnull(row['Unit']) else \"Value\"\n",
    "        ax = axes[idx]\n",
    "        df_plot = ngfs_df_iso[ngfs_df_iso['Variable'] == var]\n",
    "        if df_plot.empty:\n",
    "            ax.set_visible(False)\n",
    "            continue\n",
    "        for scenario, group in df_plot.groupby('Scenario'):\n",
    "            yvals = group[years].values\n",
    "            if yvals.shape[0] > 1:\n",
    "                yvals = yvals.mean(axis=0)\n",
    "            else:\n",
    "                yvals = yvals.flatten()\n",
    "            ax.plot(years, yvals, marker='o', label=scenario)\n",
    "        ax.set_title(f\"{var} ({input_iso})\")\n",
    "        ax.set_xlabel(\"Year\")\n",
    "        ax.set_ylabel(unit)\n",
    "        ax.legend(title=\"Scenario\")\n",
    "        ax.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(idx + 1, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "    import io\n",
    "\n",
    "    output_path = f\"output/VerveStacks_{input_iso}.xlsx\"\n",
    "\n",
    "\n",
    "    # Save the figure to a BytesIO buffer\n",
    "    imgdata = io.BytesIO()\n",
    "    fig.savefig(imgdata, format='png', bbox_inches='tight')\n",
    "    imgdata.seek(0)\n",
    "\n",
    "    # Using xlwings to add image to Excel file\n",
    "    app = xw.App(visible=False)\n",
    "    try:\n",
    "        if os.path.exists(output_path):\n",
    "            wb = app.books.open(output_path)\n",
    "        else:\n",
    "            # If file doesn't exist yet, create a new workbook\n",
    "            wb = app.books.add()\n",
    "            # Remove the default sheet if present\n",
    "            if len(wb.sheets) > 0:\n",
    "                for sheet in wb.sheets:\n",
    "                    if sheet.name.startswith('Sheet'):\n",
    "                        sheet.delete()\n",
    "\n",
    "        # Remove \"iamc_charts\" if it already exists\n",
    "        if \"iamc_charts\" in [ws.name for ws in wb.sheets]:\n",
    "            wb.sheets[\"iamc_charts\"].delete()\n",
    "\n",
    "        ws = wb.sheets.add(\"iamc_charts\")\n",
    "\n",
    "        # Save the image to a temporary file first, then insert\n",
    "        import tempfile\n",
    "        with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as temp_file:\n",
    "            fig.savefig(temp_file.name, format='png', bbox_inches='tight')\n",
    "            temp_file_path = temp_file.name\n",
    "        \n",
    "        # Insert the image\n",
    "        ws.pictures.add(temp_file_path, left=ws.range('A1').left, top=ws.range('A1').top)\n",
    "        \n",
    "        # Clean up temp file\n",
    "        os.unlink(temp_file_path)\n",
    "\n",
    "        # Save the workbook\n",
    "        wb.save(output_path)\n",
    "        wb.close()\n",
    "    finally:\n",
    "        app.quit()\n",
    "\n",
    "\n",
    "    if os.path.exists(output_path):\n",
    "        # Using xlwings to write data to Excel\n",
    "        app = xw.App(visible=False)\n",
    "        try:\n",
    "            wb = app.books.open(output_path)\n",
    "            \n",
    "            # Create or replace the sheet\n",
    "            if sheet_name in [ws.name for ws in wb.sheets]:\n",
    "                wb.sheets[sheet_name].delete()\n",
    "            \n",
    "            ws = wb.sheets.add(sheet_name)\n",
    "            \n",
    "            # Write the data\n",
    "            df_to_write = ngfs_df_iso_with_pu.sort_values(by='potential_use')\n",
    "            ws.range('A1').value = [df_to_write.columns.tolist()] + df_to_write.values.tolist()\n",
    "            \n",
    "            wb.save()\n",
    "            wb.close()\n",
    "        finally:\n",
    "            app.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c602b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile new tech characteristics from WEO data - for the appropriate region for alternative scenarios.\n",
    "def get_weo_data(input_iso):\n",
    "\n",
    "    import pandas as pd\n",
    "    from pandas.core.window.numba_ import generate_manual_numpy_nan_agg_with_axis\n",
    "\n",
    "\n",
    "    # Define paths for the uploaded files\n",
    "    weo_path = \"data/technologies/WEO_2024_PG_Assumptions_STEPSandNZE_Scenario.xlsb\"\n",
    "    tech_mapping_path = \"data/technologies/ep_technoeconomic_assumptions.xlsx\"\n",
    "    output_path = f\"output/VerveStacks_{input_iso}.xlsx\"\n",
    "\n",
    "    # Load region mapping\n",
    "    reg_map_df = pd.read_excel(tech_mapping_path, sheet_name=\"ep_regionmap\")\n",
    "    region_name = reg_map_df.loc[reg_map_df['iso'] == input_iso, 'region'].values[0]\n",
    "    \n",
    "\n",
    "    # Define relevant WEO tech assumption sheets\n",
    "    relevant_sheets = [\n",
    "        \"Renewables\",\n",
    "        \"Fossil fuels equipped with CCUS\",\n",
    "        \"Nuclear\",\n",
    "        \"Gas\",\n",
    "        \"Coal\"\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Function to process and clean each sheet\n",
    "    def process_weo_sheet(sheet_name, region_name):\n",
    "        df = pd.read_excel(weo_path, sheet_name=sheet_name, engine=\"pyxlsb\")\n",
    "\n",
    "        # Create consistent column headers\n",
    "        orig_cols = list(df.columns)\n",
    "        new_header_row = []\n",
    "        last_name = None\n",
    "        for col in orig_cols:\n",
    "            if not (str(col).startswith(\"Unnamed\") or pd.isna(col)):\n",
    "                last_name = col\n",
    "            new_header_row.append(last_name)\n",
    "        df1 = pd.concat([pd.DataFrame([new_header_row], columns=df.columns), df], ignore_index=True)\n",
    "\n",
    "        # Add attribute row\n",
    "        second_row = df1.iloc[1]\n",
    "        filled_second_row = []\n",
    "        last_val = None\n",
    "        for val in second_row:\n",
    "            if pd.notna(val):\n",
    "                last_val = val\n",
    "            filled_second_row.append(last_val)\n",
    "        df2 = pd.concat([df1.iloc[[0]], pd.DataFrame([filled_second_row], columns=df.columns), df1.iloc[1:]], ignore_index=True)\n",
    "\n",
    "        # Add Technology column\n",
    "        tech_col = []\n",
    "        current_tech = None\n",
    "        for idx, row in df2.iterrows():\n",
    "            if pd.isna(row.iloc[2]) or str(row.iloc[2]).strip() == '':\n",
    "                if pd.notna(row.iloc[0]) and str(row.iloc[0]).strip() != '':\n",
    "                    current_tech = row.iloc[0]\n",
    "            tech_col.append(current_tech)\n",
    "        df2.insert(0, \"Technology\", tech_col)\n",
    "\n",
    "        # Identify row where years start\n",
    "        year_row_idx = None\n",
    "        for idx, val in enumerate(df2.iloc[:, 2]):\n",
    "            if pd.notna(val) and str(val).isdigit() and len(str(val)) == 4:\n",
    "                year_row_idx = idx\n",
    "                break\n",
    "        if year_row_idx is None:\n",
    "            year_row_idx = 1\n",
    "\n",
    "        # Filter by region name\n",
    "        df_out = pd.concat([\n",
    "            df2.iloc[:year_row_idx+1],\n",
    "            df2.iloc[year_row_idx+1:][df2.iloc[year_row_idx+1:, 1] == region_name]\n",
    "        ], ignore_index=True)\n",
    "        return df_out\n",
    "\n",
    "\n",
    "    flat_rows = []\n",
    "\n",
    "\n",
    "    for sheet in relevant_sheets:\n",
    "        df_sheet = process_weo_sheet(sheet, region_name)\n",
    "\n",
    "        # Find the year row index for this sheet\n",
    "        year_row_idx = None\n",
    "        for idx, val in enumerate(df_sheet.iloc[:, 2]):\n",
    "            if pd.notna(val) and str(val).isdigit() and len(str(val)) == 4:\n",
    "                year_row_idx = idx\n",
    "                break\n",
    "        if year_row_idx is None:\n",
    "            year_row_idx = 1\n",
    "\n",
    "        for idx in range(year_row_idx + 1, len(df_sheet)):\n",
    "            row = df_sheet.iloc[idx]\n",
    "            technology = row['Technology']\n",
    "            for col_idx in range(2, len(df_sheet.columns)):\n",
    "                scenario = df_sheet.iloc[0, col_idx]\n",
    "                attribute = df_sheet.iloc[1, col_idx]\n",
    "                year = df_sheet.iloc[year_row_idx, col_idx] if year_row_idx < len(df_sheet) else None\n",
    "                value = row.iloc[col_idx]\n",
    "                if pd.notna(year) and pd.notna(value):\n",
    "                    flat_rows.append({\n",
    "                        'scenario': scenario,\n",
    "                        'technology': technology,\n",
    "                        'attribute': attribute,\n",
    "                        'year': year,\n",
    "                        'value': value\n",
    "                    })\n",
    "\n",
    "    flat_df = pd.DataFrame(flat_rows)\n",
    "\n",
    "\n",
    "    # There is no option in pivot_table to ignore non-numeric values directly,\n",
    "    # so we must filter to numeric values before pivoting.\n",
    "    flat_df_numeric = flat_df.copy()\n",
    "    flat_df_numeric['value'] = pd.to_numeric(flat_df_numeric['value'], errors='coerce')\n",
    "    flat_df_numeric = flat_df_numeric[flat_df_numeric['value'].notna()]\n",
    "    weo_pg_final = flat_df_numeric.pivot_table(\n",
    "        index=['scenario', 'technology', 'attribute'],\n",
    "        columns='year',\n",
    "        values='value',\n",
    "        fill_value=''\n",
    "    ).reset_index()\n",
    "\n",
    "\n",
    "    # Attempt to guess model_fuel for each technology in pivot_df by matching with df_gem_map\n",
    "    # Use approximate matching: if an exact match is not found, look for key model_fuel words in the technology string\n",
    "\n",
    "    # Build a mapping from model_fuel keywords to model_fuel from df_gem_map\n",
    "    model_fuel_keywords = (\n",
    "        df_gem_map[['model_fuel']]\n",
    "        .drop_duplicates()\n",
    "        .model_fuel\n",
    "        .dropna()\n",
    "        .unique()\n",
    "    )\n",
    "\n",
    "    # Lowercase set of model_fuel keywords for matching\n",
    "    model_fuel_keywords = [str(fuel).lower() for fuel in model_fuel_keywords if isinstance(fuel, str)]\n",
    "\n",
    "    # Create a lowercase mapping from Technology to model_fuel from df_gem_map\n",
    "    tech_to_fuel = (\n",
    "        df_gem_map.drop_duplicates(subset=['Technology', 'model_fuel'])\n",
    "        .set_index(df_gem_map['Technology'].str.lower())['model_fuel']\n",
    "        .to_dict()\n",
    "    )\n",
    "    # Add special cases for mapping technology names to model_fuel\n",
    "    special_cases = {\n",
    "        \"ccgt\": \"gas\",\n",
    "        \"ccgt + ccs\": \"gas\",\n",
    "        \"ccgt - chp\": \"gas\",\n",
    "        \"fuel cell (distributed electricity generation)\": \"hydrogen\",\n",
    "        \"igcc + ccs\": \"coal\",\n",
    "        \"marine\": \"hydro\",\n",
    "        \"Gas Turbine\": \"gas\",\n",
    "        \"oxyfuel + ccs\": \"coal\"\n",
    "    }\n",
    "\n",
    "    # Update tech_to_fuel with special cases (overriding if necessary)\n",
    "    for tech_name, model_fuel in special_cases.items():\n",
    "        tech_to_fuel[tech_name.lower()] = model_fuel\n",
    "\n",
    "\n",
    "    def guess_model_fuel(tech):\n",
    "        tech_l = str(tech).lower()\n",
    "        # 1. Try exact match\n",
    "        if tech_l in tech_to_fuel:\n",
    "            return tech_to_fuel[tech_l]\n",
    "        # 2. Try keyword match\n",
    "        for fuel in model_fuel_keywords:\n",
    "            if fuel in tech_l:\n",
    "                return fuel\n",
    "        # 3. Not found\n",
    "        return 'unknown'\n",
    "\n",
    "\n",
    "    # Map from attribute keywords to model_attribute values\n",
    "    attribute_to_model_attribute = {\n",
    "        \"O&M\": \"ncap_fom\",\n",
    "        \"capacity factor\": \"ncap_af\",\n",
    "        \"capital\": \"ncap_cost\",\n",
    "        \"construction time\": \"ncap_iled\",\n",
    "        \"efficiency\": \"efficiency\"\n",
    "    }\n",
    "\n",
    "    def get_model_attribute(attr):\n",
    "        if not isinstance(attr, str):\n",
    "            return None\n",
    "        attr_l = attr.lower()\n",
    "        for key, value in attribute_to_model_attribute.items():\n",
    "            if key.lower() in attr_l:\n",
    "                return value\n",
    "        return None\n",
    "\n",
    "    weo_pg_final['model_attribute'] = weo_pg_final['attribute'].apply(get_model_attribute)\n",
    "\n",
    "\n",
    "    weo_pg_final['model_fuel'] = weo_pg_final['technology'].apply(guess_model_fuel)\n",
    "\n",
    "    output_path = f\"output/VerveStacks_{input_iso}.xlsx\"\n",
    "\n",
    "    if os.path.exists(output_path):\n",
    "        # Using xlwings to write WEO data to Excel\n",
    "        app = xw.App(visible=False)\n",
    "        try:\n",
    "            wb = app.books.open(output_path)\n",
    "            \n",
    "            # Create or replace the sheet\n",
    "            if 'weo_pg' in [ws.name for ws in wb.sheets]:\n",
    "                wb.sheets['weo_pg'].delete()\n",
    "            \n",
    "            ws = wb.sheets.add('weo_pg')\n",
    "            \n",
    "            # Write the data\n",
    "            ws.range('A1').value = [weo_pg_final.columns.tolist()] + weo_pg_final.values.tolist()\n",
    "            \n",
    "            wb.save()\n",
    "            wb.close()\n",
    "        finally:\n",
    "            app.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67ffe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning up GEM data here. Creating the map table for key columns model_fuel and model_tech that will drive all aggregation and calibration\n",
    "# This is the first step in the process of creating the map table. Not to be used in the regular flow.\n",
    "\n",
    "\n",
    "agg_capacity = df_gem.groupby(['Type', 'Technology', 'model_fuel', 'Status'], dropna=False)['Capacity (MW)'].sum().reset_index()\n",
    "\n",
    "\n",
    "status_pivot = agg_capacity.pivot_table(\n",
    "    index=['Type', 'Technology', 'model_fuel'],\n",
    "    columns='Status',\n",
    "    values='Capacity (MW)',\n",
    "    fill_value=0\n",
    ").reset_index()\n",
    "\n",
    "display(status_pivot)\n",
    "\n",
    "\n",
    "\n",
    "# Using xlwings to write status pivot data\n",
    "app = xw.App(visible=False)\n",
    "try:\n",
    "    wb = app.books.add()\n",
    "    ws = wb.sheets[0]  # Use the default first sheet\n",
    "    ws.name = 'status_pivot'\n",
    "    \n",
    "    # Write the data\n",
    "    ws.range('A1').value = [status_pivot.columns.tolist()] + status_pivot.values.tolist()\n",
    "    \n",
    "    wb.save('output/VerveStacks_status_pivot.xlsx')\n",
    "    wb.close()\n",
    "finally:\n",
    "    app.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e8dd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccs_retrofits(input_iso,df_grouped_gem):\n",
    "\n",
    "    epa_ccs_rf_df = pd.read_excel(\"data/existing_stock/epa_coal+gas ccs retrofit data.xlsx\", sheet_name=\"epa_ccs_rf\")\n",
    "\n",
    "    duckdb.register('epa_ccs_rf_df', epa_ccs_rf_df)\n",
    "    duckdb.register('df_grouped_gem', df_grouped_gem)\n",
    "\n",
    "    # aggregated plants\n",
    "    result = duckdb.sql(\"\"\"\n",
    "        SELECT T1.model_fuel,T1.model_name || '_ccs-rf' as model_name\n",
    "        ,T2.capex,T2.fixom,T2.varom\n",
    "        ,(100-T2.heatrate_penalty) * max(T1.efficiency) / 100 AS efficiency\n",
    "        ,(100-T2.capacity_penalty) * .95 / 100 AS AF\n",
    "        ,max(T1.efficiency) as efficiency_old\n",
    "        ,T1.model_name AS plant_old\n",
    "        FROM df_grouped_gem T1 \n",
    "        inner join epa_ccs_rf_df T2 ON \n",
    "        T1.model_fuel=T2.model_fuel and\n",
    "        T2.eff1 = 0\n",
    "        and\n",
    "        T2.cap1=0\n",
    "\n",
    "        where T1.model_description ilike 'aggregated%' and T1.model_fuel='coal'\n",
    "\n",
    "        group by T1.model_fuel,T1.model_name,T2.capex,T2.fixom,T2.varom,T2.heatrate_penalty,T2.capacity_penalty,T1.model_name\n",
    "\n",
    "        UNION\n",
    "        \n",
    "        SELECT T1.model_fuel,T1.model_name || '_ccs-rf' as model_name\n",
    "        ,T2.capex,T2.fixom,T2.varom\n",
    "        ,(100-T2.heatrate_penalty) * T1.efficiency / 100 AS efficiency\n",
    "        ,(100-T2.capacity_penalty) * .95 / 100 AS AF\n",
    "        ,T1.efficiency as efficiency_old\n",
    "        ,T1.model_name AS plant_old\n",
    "        FROM df_grouped_gem T1 \n",
    "        inner join epa_ccs_rf_df T2 ON \n",
    "        T1.model_fuel=T2.model_fuel and\n",
    "        T1.efficiency < T2.efficiency and T1.efficiency >= T2.eff1\n",
    "        and\n",
    "        T1.capacity_gw < T2.capacity/1000 and T1.capacity_gw >= T2.cap1/1000\n",
    "\n",
    "        where not T1.model_description ilike 'aggregated%' and T1.model_fuel='coal'\n",
    "\n",
    "        UNION\n",
    "\n",
    "        SELECT T1.model_fuel,T1.model_name || '_ccs-rf' as model_name\n",
    "        ,T2.capex,T2.fixom,T2.varom\n",
    "        ,(100-T2.heatrate_penalty) * max(T1.efficiency) / 100 AS efficiency\n",
    "        ,(100-T2.capacity_penalty) * .95 / 100 AS AF\n",
    "        ,max(T1.efficiency) as efficiency_old\n",
    "        ,T1.model_name AS plant_old\n",
    "        FROM df_grouped_gem T1 \n",
    "        inner join epa_ccs_rf_df T2 ON \n",
    "        T2.model_fuel='gas'\n",
    "        \n",
    "        where T1.model_fuel IN ('gas','oil')\n",
    "\n",
    "        group by T1.model_fuel,T1.model_name,T2.capex,T2.fixom,T2.varom,T2.heatrate_penalty,T2.capacity_penalty,T1.model_name\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        ).df()\n",
    "\n",
    "    output_path = f\"output/VerveStacks_{input_iso}.xlsx\"\n",
    "    \n",
    "    if os.path.exists(output_path):\n",
    "        # Using xlwings to write CCS retrofits data\n",
    "        app = xw.App(visible=False)\n",
    "        try:\n",
    "            wb = app.books.open(output_path)\n",
    "            \n",
    "            # Create or replace the sheet\n",
    "            if 'ccs_retrofits' in [ws.name for ws in wb.sheets]:\n",
    "                wb.sheets['ccs_retrofits'].delete()\n",
    "            \n",
    "            ws = wb.sheets.add('ccs_retrofits')\n",
    "            \n",
    "            # Write the data\n",
    "            result_sorted = result.sort_values(by='model_name')\n",
    "            ws.range('A1').value = [result_sorted.columns.tolist()] + result_sorted.values.tolist()\n",
    "            \n",
    "            wb.save()\n",
    "            wb.close()\n",
    "        finally:\n",
    "            app.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d3edd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_targets_ember(input_iso):\n",
    "\n",
    "\n",
    "    ember_raw_data_long = pd.read_excel(\"data/ember_targets_download2025jul.xlsx\", sheet_name=\"raw_data_long\")\n",
    "    ember_sources = pd.read_excel(\"data/ember_targets_download2025jul.xlsx\", sheet_name=\"sources\")\n",
    "\n",
    "    duckdb.register('ember_raw_data_long', ember_raw_data_long)\n",
    "    duckdb.register('ember_sources', ember_sources)\n",
    "\n",
    "    result = duckdb.sql(f\"\"\"\n",
    "\n",
    "    select T1.TARGET_YEAR,T1.FUEL_CATEGORY,T1.METRIC,T1.VALUE,\n",
    "    T2.SOURCE_TYPE,T2.SOURCE_NAME,T2.PUBLISHER,T2.ANNOUNCEMENT_DATE,T2.LINK,T2.SOURCE_SUMMARY,T2.NOTES\n",
    "    from ember_raw_data_long T1\n",
    "    inner join ember_sources T2 on T1.SOURCE_ID=T2.SOURCE_ID\n",
    "\n",
    "    where T1.COUNTRY_CODE='{input_iso}'\n",
    "    order by T2.SOURCE_TYPE,T1.METRIC,T1.FUEL_CATEGORY\n",
    "\n",
    "        \"\"\"\n",
    "        ).df()\n",
    "\n",
    "\n",
    "    output_path = f\"output/VerveStacks_{input_iso}.xlsx\"\n",
    "        \n",
    "    if os.path.exists(output_path):\n",
    "        # Using xlwings to write RE targets data\n",
    "        app = xw.App(visible=False)\n",
    "        try:\n",
    "            wb = app.books.open(output_path)\n",
    "            \n",
    "            # Create or replace the sheet\n",
    "            if 're_targets' in [ws.name for ws in wb.sheets]:\n",
    "                wb.sheets['re_targets'].delete()\n",
    "            \n",
    "            ws = wb.sheets.add('re_targets')\n",
    "            \n",
    "            # Write the data\n",
    "            ws.range('A1').value = [result.columns.tolist()] + result.values.tolist()\n",
    "            \n",
    "            wb.save()\n",
    "            wb.close()\n",
    "        finally:\n",
    "            app.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f7f974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Veda model folder\n",
    "\n",
    "input_iso='NGA'\n",
    "\n",
    "from atexit import register\n",
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import openpyxl\n",
    "\n",
    "def copy_vs_iso_template(input_iso):\n",
    "    src_folder = \"assumptions/VerveStacks_ISO_template\"\n",
    "    dest_folder = f\"output/VerveStacks_{input_iso}\"\n",
    "\n",
    "    # If destination folder exists, delete it\n",
    "    if os.path.exists(dest_folder):\n",
    "        shutil.rmtree(dest_folder)\n",
    "    # Copy the template folder to the destination\n",
    "    shutil.copytree(src_folder, dest_folder)\n",
    "\n",
    "copy_vs_iso_template(input_iso)\n",
    "dest_folder = f\"output/VerveStacks_{input_iso}\"\n",
    "\n",
    "syssettings_path = os.path.join(dest_folder, \"SysSettings.xlsx\")\n",
    "wb_sys = openpyxl.load_workbook(syssettings_path)\n",
    "ws_sys = wb_sys[\"system_settings\"]\n",
    "ws_sys[\"B3\"] = input_iso\n",
    "wb_sys.save(syssettings_path)\n",
    "\n",
    "existing_stock_df = pd.read_excel(f\"output/vervestacks_{input_iso}.xlsx\", sheet_name=\"existing_stock\")\n",
    "ccs_retrofits_df = pd.read_excel(f\"output/vervestacks_{input_iso}.xlsx\", sheet_name=\"ccs_retrofits\")\n",
    "weo_pg_df = pd.read_excel(f\"output/vervestacks_{input_iso}.xlsx\", sheet_name=\"weo_pg\")\n",
    "iamc_data_df = pd.read_excel(f\"output/vervestacks_{input_iso}.xlsx\", sheet_name=\"iamc_data\")\n",
    "base_year_data_df = pd.read_excel(f\"output/vervestacks_{input_iso}.xlsx\", sheet_name=\"base_year_data\")\n",
    "life_df = pd.read_excel(f\"data/technologies/ep_technoeconomic_assumptions.xlsx\", sheet_name=\"life\")\n",
    "\n",
    "duckdb.register('existing_stock_df', existing_stock_df)\n",
    "duckdb.register('ccs_retrofits_df', ccs_retrofits_df)\n",
    "duckdb.register('weo_pg_df', weo_pg_df)\n",
    "duckdb.register('life_df', life_df)\n",
    "\n",
    "fi_t = duckdb.sql(f\"\"\"\n",
    "select T1.model_name AS process,T1.model_fuel AS \"comm-in\",\n",
    "CASE\n",
    "    WHEN\n",
    "    T1.model_fuel = 'solar'\n",
    "    THEN 'ELC_Sol-' || '{input_iso}'\n",
    "    WHEN\n",
    "    T1.model_fuel = 'wind'\n",
    "    THEN 'ELC_Win-' || '{input_iso}'\n",
    "    ELSE 'ELC'\n",
    "END AS \"comm-out\",\n",
    "T1.\"Start year\" AS year,T1.capacity_gw AS ncap_pasti,\n",
    "T1.efficiency AS efficiency,T1.capex AS ncap_cost,T1.fixom AS ncap_fom,T1.varom AS act_cost,\n",
    "case\n",
    "    when\n",
    "    coalesce(T1.retirement_year,2055) - T1.\"Start year\" > T2.life\n",
    "    then\n",
    "    coalesce(T1.retirement_year,2055) - T1.\"Start year\" \n",
    "    else\n",
    "    T2.life\n",
    "end AS ncap_tlife\n",
    "from existing_stock_df T1\n",
    "left join life_df T2 on T1.model_name ilike T2.model_name || '%'\n",
    "order by T1.model_name,T1.\"Start year\"\n",
    "\"\"\").df()\n",
    "\n",
    "fi_t.loc[fi_t['process'].str.startswith('ep_hydro_ps'), 'comm-in'] = 'ELC'\n",
    "\n",
    "\n",
    "fi_p = duckdb.sql(\"\"\"\n",
    "select \n",
    "'ele' AS set,\n",
    "T1.model_name AS process,T1.model_description AS description,\n",
    "'GW' AS capacity_unit,\n",
    "'TWh' AS activity_unit,\n",
    "CASE when model_fuel IN ('solar','windon','windoff') then 'annual' else 'daynite' end AS timeslicelevel,\n",
    "CASE when model_description ilike 'aggregated%' then 'yes' else 'no' end AS vintage\n",
    "\n",
    "from existing_stock_df T1\n",
    "group by T1.model_name,T1.model_description,T1.model_fuel\n",
    "order by T1.model_name\n",
    "\"\"\").df()\n",
    "\n",
    "fi_p.loc[fi_p['process'].str.startswith('ep_hydro_ps'), 'set'] = 'STG'\n",
    "\n",
    "duckdb.register('fi_p', fi_p)\n",
    "\n",
    "fi_t_ccs = duckdb.sql(\"\"\"\n",
    "select T1.model_name AS process,T1.model_fuel AS \"comm-in\",'ELC' as \"comm-out\",\n",
    "T1.efficiency AS efficiency,T1.capex AS ncap_cost,T1.fixom AS ncap_fom,T1.varom AS act_cost,\n",
    "T1.AF AS AF,\n",
    "T1.plant_old AS other_indexes, 1 AS prc_refit,\n",
    "20 AS ncap_tlife\n",
    "from ccs_retrofits_df T1\n",
    "\n",
    "order by T1.model_name\n",
    "\"\"\").df()\n",
    "\n",
    "fi_p_ccs = duckdb.sql(\"\"\"\n",
    "select \n",
    "'ele' AS set,\n",
    "T1.model_name AS process,'ccs retrofit of -- ' || T2.description AS description,\n",
    "'GW' AS capacity_unit,\n",
    "'TWh' AS activity_unit,\n",
    "'daynite' AS timeslicelevel,\n",
    "'no' AS vintage\n",
    "\n",
    "from ccs_retrofits_df T1\n",
    "inner join fi_p T2 ON T1.plant_old = T2.process\n",
    "group by T1.model_name,T2.description\n",
    "order by T1.model_name\n",
    "\"\"\").df()\n",
    "\n",
    "fi_t_weo = duckdb.sql(\"\"\"\n",
    "select T1.technology AS process,T1.model_fuel AS \"comm-in\",'ELC' as \"comm-out\",\n",
    "\"2023\",\"2030\",\"2050\",\n",
    "T1.model_attribute AS attribute,\n",
    "from weo_pg_df T1\n",
    "where T1.scenario ilike 'Stated%'\n",
    "order by T1.technology,T1.model_attribute\n",
    "\"\"\").df()\n",
    "\n",
    "fi_p_weo = duckdb.sql(\"\"\"\n",
    "select \n",
    "'ele' AS set,\n",
    "T1.technology AS process,'' AS description,\n",
    "'GW' AS capacity_unit,\n",
    "'TWh' AS activity_unit,\n",
    "'daynite' AS timeslicelevel,\n",
    "'yes' AS vintage\n",
    "\n",
    "from weo_pg_df T1\n",
    "group by T1.technology\n",
    "order by T1.technology\n",
    "\"\"\").df()\n",
    "\n",
    "\n",
    "# Delete the file if already present, then create vt_vervestacks_{iso}.xlsx in the destination folder\n",
    "vt_output_path = os.path.join(dest_folder, f\"vt_vervestacks_{input_iso}_v1.xlsx\")\n",
    "if os.path.exists(vt_output_path):\n",
    "    os.remove(vt_output_path)\n",
    "\n",
    "# Using xlwings to write complex multi-table Excel file\n",
    "app = xw.App(visible=False)\n",
    "try:\n",
    "    wb = app.books.add()\n",
    "    \n",
    "    # Create existing_stock sheet\n",
    "    if wb.sheets[0].name.startswith('Sheet'):\n",
    "        wb.sheets[0].name = 'existing_stock'\n",
    "    ws_existing = wb.sheets[0]\n",
    "    \n",
    "    # Write fi_t data starting at row 3 (1-based)\n",
    "    ws_existing.range('A3').value = [fi_t.columns.tolist()] + fi_t.values.tolist()\n",
    "    \n",
    "    # Write fi_p data starting at same row but different column\n",
    "    start_col = fi_t.shape[1] + 3  # +2 for gap, +1 for next column\n",
    "    ws_existing.range(f'{chr(64 + start_col)}3').value = [fi_p.columns.tolist()] + fi_p.values.tolist()\n",
    "    \n",
    "    # Add labels\n",
    "    ws_existing.range('A2').value = \"~fi_t\"\n",
    "    ws_existing.range(f'{chr(64 + start_col)}2').value = \"~fi_process\"\n",
    "    \n",
    "    # Create ccs_retrofits sheet\n",
    "    ws_ccs = wb.sheets.add('ccs_retrofits')\n",
    "    \n",
    "    # Write fi_t_ccs data starting at row 3\n",
    "    ws_ccs.range('A3').value = [fi_t_ccs.columns.tolist()] + fi_t_ccs.values.tolist()\n",
    "    \n",
    "    # Write fi_p_ccs data starting at same row but different column\n",
    "    start_col_ccs = fi_t_ccs.shape[1] + 3\n",
    "    ws_ccs.range(f'{chr(64 + start_col_ccs)}3').value = [fi_p_ccs.columns.tolist()] + fi_p_ccs.values.tolist()\n",
    "    \n",
    "    # Add labels\n",
    "    ws_ccs.range('A2').value = \"~fi_t\"\n",
    "    ws_ccs.range(f'{chr(64 + start_col_ccs)}2').value = \"~fi_process\"\n",
    "    \n",
    "    # Create weo_pg sheet\n",
    "    ws_weo = wb.sheets.add('weo_pg')\n",
    "    \n",
    "    # Write fi_t_weo data starting at row 3\n",
    "    ws_weo.range('A3').value = [fi_t_weo.columns.tolist()] + fi_t_weo.values.tolist()\n",
    "    \n",
    "    # Write fi_p_weo data starting at same row but different column\n",
    "    start_col_weo = fi_t_weo.shape[1] + 3\n",
    "    ws_weo.range(f'{chr(64 + start_col_weo)}3').value = [fi_p_weo.columns.tolist()] + fi_p_weo.values.tolist()\n",
    "    \n",
    "    # Add labels\n",
    "    ws_weo.range('A2').value = \"~fi_t\"\n",
    "    ws_weo.range(f'{chr(64 + start_col_weo)}2').value = \"~fi_process\"\n",
    "    \n",
    "    wb.save(vt_output_path)\n",
    "    wb.close()\n",
    "finally:\n",
    "    app.quit()\n",
    "    \n",
    "\n",
    "vt_output_path = os.path.join(dest_folder, f\"SuppXLS/Scen_Par-NGFS.xlsx\")\n",
    "\n",
    "\n",
    "# Using xlwings to add sheets to existing workbook\n",
    "if os.path.exists(vt_output_path):\n",
    "    app = xw.App(visible=False)\n",
    "    try:\n",
    "        wb = app.books.open(vt_output_path)\n",
    "        \n",
    "        # Add or replace iamc_data sheet\n",
    "        if \"iamc_data\" in [ws.name for ws in wb.sheets]:\n",
    "            wb.sheets[\"iamc_data\"].clear()\n",
    "        wb.sheets[\"iamc_data\"].range('A1').value = [iamc_data_df.columns.tolist()] + iamc_data_df.values.tolist()\n",
    "        \n",
    "        # Add or replace base_year_data sheet\n",
    "        if \"base_year_data\" in [ws.name for ws in wb.sheets]:\n",
    "            wb.sheets[\"base_year_data\"].clear()\n",
    "        wb.sheets[\"base_year_data\"].range('A1').value = [base_year_data_df.columns.tolist()] + base_year_data_df.values.tolist()\n",
    "        \n",
    "        wb.save()\n",
    "        wb.close()\n",
    "    finally:\n",
    "        app.quit()\n",
    "\n",
    "bvs_output_path = os.path.join(dest_folder, f\"SuppXLS/Scen_Base_VS.xlsx\")\n",
    "\n",
    "# Using xlwings to write to second Excel file\n",
    "if os.path.exists(bvs_output_path):\n",
    "    app = xw.App(visible=False)\n",
    "    try:\n",
    "        wb = app.books.open(bvs_output_path)\n",
    "        \n",
    "        # Add or replace base_year_data sheet\n",
    "        if \"base_year_data\" in [ws.name for ws in wb.sheets]:\n",
    "            wb.sheets[\"base_year_data\"].clear()\n",
    "        wb.sheets[\"base_year_data\"].range('A1').value = [base_year_data_df.columns.tolist()] + base_year_data_df.values.tolist()\n",
    "        \n",
    "        wb.save()\n",
    "        wb.close()\n",
    "    finally:\n",
    "        app.quit()\n",
    "\n",
    "\n",
    "# clean up the RE resource SubRES file\n",
    "def remove_other_iso(input_iso,file_name):\n",
    "\n",
    "    RE_SubRES_file = f\"output/VerveStacks_{input_iso}/\" + file_name\n",
    "    \n",
    "    import xlwings as xw\n",
    "\n",
    "    app = xw.App(visible=False)\n",
    "    try:\n",
    "        wb = app.books.open(RE_SubRES_file)\n",
    "\n",
    "        # Fix: skip rows where cell is None or not a 3-char string\n",
    "        for ws in wb.sheets:\n",
    "\n",
    "            cell_a5 = ws.range('A5').value\n",
    "            if cell_a5 is None or (isinstance(cell_a5, float) and pd.isna(cell_a5)):\n",
    "                continue  # go to next sheet if A5 is null\n",
    "\n",
    "            row = 5\n",
    "            max_row = ws.used_range.last_cell.row if ws.used_range else 5\n",
    "            while row <= max_row:\n",
    "                cell = ws.range(f'A{row}').value\n",
    "                # Only process if cell is a 3-char string\n",
    "                if isinstance(cell, str) and len(cell) == 3:\n",
    "                    if cell != input_iso:\n",
    "                        ws.range(f'{row}:{row}').delete()\n",
    "                        max_row -= 1  # since a row was deleted\n",
    "                        continue\n",
    "                row += 1\n",
    "\n",
    "        wb.save()\n",
    "        wb.close()\n",
    "    finally:\n",
    "        app.quit()\n",
    "\n",
    "print(\"Removing other ISOs from SubRES_REZoning_Sol_Win_andHydro.xlsx\")\n",
    "remove_other_iso(input_iso,'SubRES_Tmpl/SubRES_REZoning_Sol_Win_andHydro.xlsx')\n",
    "\n",
    "\n",
    "def replace_kinesys_name(input_iso,file_name,sheet_name,kinesys_name):\n",
    "\n",
    "    scen_file = f\"output/VerveStacks_{input_iso}/\" + file_name\n",
    "\n",
    "    # Using xlwings to replace kinesys names\n",
    "    app = xw.App(visible=False)\n",
    "    try:\n",
    "        wb = app.books.open(scen_file)\n",
    "        ws = wb.sheets[sheet_name]\n",
    "        \n",
    "        ws.range('A1').value = input_iso        # to clean up other ISOs\n",
    "\n",
    "        # Iterate over all columns in row 4 (xlwings uses 1-based indexing)\n",
    "        if ws.used_range:\n",
    "            max_col = ws.used_range.last_cell.column\n",
    "            for col in range(1, max_col + 1):\n",
    "                cell_value = ws.range((4, col)).value\n",
    "                if cell_value and str(cell_value).lower() == kinesys_name.lower():\n",
    "                    ws.range((4, col)).value = input_iso\n",
    "\n",
    "        # Save the workbook\n",
    "        wb.save()\n",
    "        wb.close()\n",
    "    finally:\n",
    "        app.quit()\n",
    "\n",
    "df_kinesys_region_map = pd.read_excel(\"assumptions/VS_mappings.xlsx\", sheet_name=\"kinesys_region_map\")\n",
    "df_kinesys_region_map = df_kinesys_region_map[df_kinesys_region_map['iso']==input_iso]\n",
    "\n",
    "print(\"Replacing kinesys name in Scen_TSParameters_12.xlsx\")\n",
    "replace_kinesys_name(input_iso,'SuppXLS/Scen_TSParameters_12.xlsx','PKFLX',df_kinesys_region_map['kinesys'].iloc[0])\n",
    "replace_kinesys_name(input_iso,'SuppXLS/Scen_TSParameters_12.xlsx','YRFRs',df_kinesys_region_map['kinesys'].iloc[0])\n",
    "\n",
    "print(\"Replacing kinesys name in Scen_TSParameters_108.xlsx\")\n",
    "replace_kinesys_name(input_iso,'SuppXLS/Scen_TSParameters_108.xlsx','PKFLX',df_kinesys_region_map['kinesys'].iloc[0])\n",
    "replace_kinesys_name(input_iso,'SuppXLS/Scen_TSParameters_108.xlsx','YRFRs',df_kinesys_region_map['kinesys'].iloc[0])\n",
    "\n",
    "print(\"Replacing kinesys name in Scen_TSParameters_348.xlsx\")\n",
    "replace_kinesys_name(input_iso,'SuppXLS/Scen_TSParameters_348.xlsx','PKFLX',df_kinesys_region_map['kinesys'].iloc[0])\n",
    "replace_kinesys_name(input_iso,'SuppXLS/Scen_TSParameters_348.xlsx','YRFRs',df_kinesys_region_map['kinesys'].iloc[0])\n",
    "\n",
    "import xlwings as xw\n",
    "\n",
    "# # List of Excel files written to in this block\n",
    "# excel_files_to_update = [\n",
    "#     'SubRES_Tmpl/SubRES_REZoning_Sol_Win_andHydro.xlsx',\n",
    "#     f\"output/VerveStacks_{input_iso}/SuppXLS/Scen_TSParameters_12.xlsx\",\n",
    "#     f\"output/VerveStacks_{input_iso}/SuppXLS/Scen_TSParameters_108.xlsx\",\n",
    "#     f\"output/VerveStacks_{input_iso}/SuppXLS/Scen_TSParameters_348.xlsx\",\n",
    "#     syssettings_path,\n",
    "#     bvs_output_path\n",
    "# ]\n",
    "\n",
    "# # Remove duplicates and ensure files exist before opening\n",
    "# unique_excel_files = set()\n",
    "# for file in excel_files_to_update:\n",
    "#     if os.path.exists(file):\n",
    "#         unique_excel_files.add(os.path.abspath(file))\n",
    "\n",
    "# for file_path in unique_excel_files:\n",
    "#     app = xw.App(visible=False)\n",
    "#     try:\n",
    "#         wb = app.books.open(file_path)\n",
    "#         wb.app.calculate()  # Calculate all formulas in the workbook\n",
    "#         wb.save()\n",
    "#         wb.close()\n",
    "#     finally:\n",
    "#         app.quit()\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2744cfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detailed characteristics of existing stock for a given country\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import shutil\n",
    "import xlwings as xw\n",
    "\n",
    "import duckdb\n",
    "\n",
    "\n",
    "input_iso='CHE'\n",
    "capacity_threshold=300\n",
    "\n",
    "\"\"\"\n",
    "For a given ISO code, compute utilization factors from IRENA and EMBER,\n",
    "and export both tables to a single Excel file (two sheets, one for each source).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "template_path = \"assumptions/vervestacks_ISO_template.xlsx\"\n",
    "output_path = f\"output/vervestacks_{input_iso}.xlsx\"\n",
    "\n",
    "# Define a helper function to read a value from a given cell using xlwings\n",
    "def read_cell(ws, cell):\n",
    "    value = ws.range(cell).value\n",
    "    return value\n",
    "\n",
    "\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    app = xw.App(visible=False)\n",
    "    try:\n",
    "        wb = app.books.open(output_path)\n",
    "        ws = wb.sheets['system_settings']\n",
    "        efficiency_adjustment_gas = ws.range('A4').value\n",
    "        efficiency_adjustment_coal = ws.range('A5').value\n",
    "        capacity_threshold = ws.range('A3').value\n",
    "        wb.close()\n",
    "    finally:\n",
    "        app.quit()\n",
    "else:\n",
    "\n",
    "    # If the output file already exists and is open in Excel, shutil.copyfile may fail.\n",
    "    # To handle this, try-except and if it fails, create a new workbook from template.\n",
    "    try:\n",
    "        shutil.copyfile(template_path, output_path)\n",
    "        # Open the newly created Excel file using xlwings\n",
    "        app = xw.App(visible=False)\n",
    "        try:\n",
    "            wb = app.books.open(output_path)\n",
    "\n",
    "            # Select the 'system_settings' sheet\n",
    "            if 'system_settings' in [ws.name for ws in wb.sheets]:\n",
    "                ws = wb.sheets['system_settings']\n",
    "            else:\n",
    "                ws = wb.sheets[0]  # fallback if sheet not found\n",
    "\n",
    "            # Write input_iso to cell A1\n",
    "            ws.range('A1').value = input_iso\n",
    "            ws.range('A3').value = capacity_threshold\n",
    "\n",
    "            # Read variables from the sheet\n",
    "            efficiency_adjustment_gas = read_cell(ws, 'A4')\n",
    "            efficiency_adjustment_coal = read_cell(ws, 'A5')\n",
    "\n",
    "            # Save the workbook\n",
    "            wb.save()\n",
    "            wb.close()\n",
    "        finally:\n",
    "            app.quit()\n",
    "\n",
    "    except PermissionError as e:\n",
    "        print(f\"PermissionError: {e}\")\n",
    "        print(f\"Could not overwrite {output_path}. Please close the file if it is open in Excel and try again.\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error copying template: {e}\")\n",
    "        # As a fallback, create a new workbook from the template using xlwings\n",
    "        try:\n",
    "            app = xw.App(visible=False)\n",
    "            try:\n",
    "                wb_template = app.books.open(template_path)\n",
    "                wb_template.save(output_path)\n",
    "                wb_template.close()\n",
    "                print(f\"Created {output_path} from template using xlwings.\")\n",
    "            finally:\n",
    "                app.quit()\n",
    "        except Exception as e2:\n",
    "            print(f\"Failed to create workbook from template: {e2}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "For a given ISO code, compute utilization factors from IRENA and EMBER,\n",
    "and export both tables to a single Excel file (two sheets, one for each source).\n",
    "\"\"\"\n",
    "\n",
    "# --- IRENA Utilization Factors ---\n",
    "\n",
    "# When filtering for iso, groupby fields iso_code, model_fuel, and year\n",
    "df_irena_c_iso = (\n",
    "    df_irena_c[df_irena_c['iso_code'] == input_iso]\n",
    "    .groupby(['iso_code', 'model_fuel', 'Year'], as_index=False)\n",
    "    .agg({'Electricity statistics (MW/GWh)': lambda x: x.sum() / 1000})\n",
    ")\n",
    "df_irena_g_iso = (\n",
    "    df_irena_g[df_irena_g['iso_code'] == input_iso]\n",
    "    .groupby(['iso_code', 'model_fuel', 'Year'], as_index=False)\n",
    "    .agg({'Electricity statistics (MW/GWh)': lambda x: x.sum() / 1000})\n",
    ")\n",
    "\n",
    "# Standardize column names for merging\n",
    "df_irena_c_iso = df_irena_c_iso.rename(columns={\n",
    "    'Electricity statistics (MW/GWh)': 'Capacity_GW'\n",
    "})\n",
    "df_irena_g_iso = df_irena_g_iso.rename(columns={\n",
    "    'Electricity statistics (MW/GWh)': 'Generation_TWh'\n",
    "})\n",
    "\n",
    "# Merge on Country, Type, Year\n",
    "df_irena_util = pd.merge(\n",
    "    df_irena_c_iso[['iso_code', 'model_fuel', 'Year', 'Capacity_GW']],\n",
    "    df_irena_g_iso[['iso_code', 'model_fuel', 'Year', 'Generation_TWh']],\n",
    "    on=['iso_code', 'model_fuel', 'Year'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Compute utilization factor: (Generation in GWh) / (Capacity in MW * 8.76)\n",
    "df_irena_util['utilization_factor'] = df_irena_util['Generation_TWh'] / (df_irena_util['Capacity_GW'] * 8.76)\n",
    "\n",
    "# Pivot table: Type x Year, values = Utilization_Factor\n",
    "util_pivot_irena = df_irena_util.pivot_table(\n",
    "    index=['iso_code', 'model_fuel'],\n",
    "    columns='Year',\n",
    "    values='utilization_factor'\n",
    ")\n",
    "\n",
    "# --- EMBER Utilization Factors ---\n",
    "\n",
    "# Get capacity (GW) by country code, year, Type\n",
    "# When creating cap and gen dfs, sum Value on the 3 indexes (iso_code, Year, model_fuel)\n",
    "df_capacity = df_ember[\n",
    "    (df_ember['Unit'] == 'GW') & \n",
    "    (df_ember['iso_code'] == input_iso)\n",
    "].copy()\n",
    "df_capacity = (\n",
    "    df_capacity\n",
    "    .groupby(['iso_code', 'Year', 'model_fuel'], as_index=False)['Value']\n",
    "    .sum()\n",
    ")\n",
    "df_capacity = df_capacity.rename(columns={'Value': 'Capacity_GW'})\n",
    "\n",
    "df_generation = df_ember[\n",
    "    (df_ember['Unit'] == 'TWh') & \n",
    "    (df_ember['iso_code'] == input_iso)\n",
    "].copy()\n",
    "df_generation = (\n",
    "    df_generation\n",
    "    .groupby(['iso_code', 'model_fuel', 'Year'], as_index=False)['Value']\n",
    "    .sum()\n",
    ")\n",
    "df_generation = df_generation.rename(columns={'Value': 'Generation_TWh'})\n",
    "# Merge on Country code, Year, Type\n",
    "df_ember_util = pd.merge(\n",
    "    df_capacity,\n",
    "    df_generation[['iso_code', 'model_fuel', 'Year', 'Generation_TWh']],\n",
    "    on=['iso_code', 'model_fuel', 'Year'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Compute utilization factor: (Generation in GWh) / (Capacity in GW * 8760)\n",
    "df_ember_util['utilization_factor'] = df_ember_util['Generation_TWh'] / (df_ember_util['Capacity_GW'] * 8.76)\n",
    "# Pivot table for easier viewing\n",
    "util_pivot_ember = df_ember_util.pivot_table(\n",
    "    index=['iso_code', 'model_fuel'],\n",
    "    columns='Year',\n",
    "    values='utilization_factor'\n",
    ")\n",
    "\n",
    "\n",
    "# existing stock characteristics\n",
    "#############################################################################################################################\n",
    "costs_df = pd.read_excel(\"data/technologies/ep_technoeconomic_assumptions.xlsx\", sheet_name=\"costs\")\n",
    "costs_size_multipliers_df = pd.read_excel(\"data/technologies/ep_technoeconomic_assumptions.xlsx\", sheet_name=\"costs_size_multipliers\")\n",
    "reg_mult_df = pd.read_excel(\"data/technologies/ep_technoeconomic_assumptions.xlsx\", sheet_name=\"regional_multipliers\")\n",
    "reg_map_df = pd.read_excel(\"data/technologies/ep_technoeconomic_assumptions.xlsx\", sheet_name=\"ep_regionmap\")\n",
    "thermal_eff_df = pd.read_excel(\"data/technologies/ep_technoeconomic_assumptions.xlsx\", sheet_name=\"thermal_eff\")\n",
    "re_units_cf_grid_cell_mapping = pd.read_csv(\"data/GlobalEnergyMonitor/re_units_cf_grid_cell_mapping.csv\")\n",
    "\n",
    "# Example: View costs_df for a particular model_name using DuckDB SQL\n",
    "\n",
    "# Register the DataFrame as a DuckDB table\n",
    "duckdb.register('costs_df', costs_df)\n",
    "duckdb.register('costs_size_multipliers_df', costs_size_multipliers_df)\n",
    "duckdb.register('reg_mult_df', reg_mult_df)\n",
    "duckdb.register('reg_map_df', reg_map_df)\n",
    "duckdb.register('thermal_eff_df', thermal_eff_df)\n",
    "\n",
    "\n",
    "def get_costs_and_eff(input_iso, input_size, input_model_name, input_year):\n",
    "    # Get all year columns from thermal_eff_df that are less than input_year\n",
    "    year_cols = [col for col in thermal_eff_df.columns if str(col).isdigit() and int(col) < input_year]\n",
    "    if year_cols:\n",
    "        max_year = max(year_cols, key=lambda x: int(x))\n",
    "    else:\n",
    "        max_year = min([col for col in thermal_eff_df.columns if str(col).isdigit()], key=lambda x: int(x))\n",
    "\n",
    "    # Check if model_name exists in thermal_eff_df\n",
    "    eff_rows = thermal_eff_df[thermal_eff_df['model_name'].str.lower() == input_model_name.lower()]\n",
    "    if eff_rows.empty:\n",
    "        # No efficiency data for this model_name: skip the efficiency join, set efficiency=1\n",
    "        result = duckdb.query(f\"\"\"\n",
    "            SELECT \n",
    "                T1.capex * T2.capex * T3.capex as capex, \n",
    "                T1.fixom * T2.fixom * T3.fixom as fixom, \n",
    "                T1.varom * T2.varom * T3.varom as varom, \n",
    "                1 as efficiency\n",
    "            FROM\n",
    "            (SELECT * from costs_df WHERE lower(model_name) = lower('{input_model_name}')) T1\n",
    "            CROSS JOIN\n",
    "            (SELECT * from costs_size_multipliers_df WHERE size = (SELECT max(size) from costs_size_multipliers_df where size < {input_size})) T2\n",
    "            CROSS JOIN\n",
    "            (Select T1.* from reg_mult_df T1 INNER JOIN reg_map_df T2 ON T1.region = T2.region WHERE T2.iso = '{input_iso}') T3\n",
    "        \"\"\").to_df()\n",
    "    else:\n",
    "        # Query the table for the particular model_name, including efficiency\n",
    "        result = duckdb.query(f\"\"\"\n",
    "            SELECT \n",
    "                T1.capex * T2.capex * T3.capex as capex, \n",
    "                T1.fixom * T2.fixom * T3.fixom as fixom, \n",
    "                T1.varom * T2.varom * T3.varom as varom, \n",
    "                T4.efficiency * T3.efficiency as efficiency\n",
    "            FROM\n",
    "            (SELECT * from costs_df WHERE lower(model_name) = lower('{input_model_name}')) T1\n",
    "            CROSS JOIN\n",
    "            (SELECT * from costs_size_multipliers_df WHERE size = (SELECT max(size) from costs_size_multipliers_df where size < {input_size})) T2\n",
    "            CROSS JOIN\n",
    "            (Select T1.* from reg_mult_df T1 INNER JOIN reg_map_df T2 ON T1.region = T2.region WHERE T2.iso = '{input_iso}') T3\n",
    "            CROSS JOIN\n",
    "            (SELECT \"{max_year}\" as efficiency from thermal_eff_df where lower(model_name) = lower('{input_model_name}') AND size = (SELECT max(size) from thermal_eff_df where size < {input_size})) T4\n",
    "        \"\"\").to_df()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Filter df_gem for a particular iso, e.g., 'USA'\n",
    "df_gem_iso = df_gem[df_gem['iso_code'] == input_iso]\n",
    "\n",
    "\n",
    "# Keep only rows from df_gem_iso where 'Status' is in a certain list of values\n",
    "statuses_to_keep = ['operating', 'construction', 'mothballed']  # example statuses to keep\n",
    "df_gem_iso = df_gem_iso[df_gem_iso['Status'].isin(statuses_to_keep)]\n",
    "\n",
    "df_gem_iso.to_excel(\"output/df_gem_iso_before_gap_filling.xlsx\", index=False)\n",
    "\n",
    "def is_valid_year(val):\n",
    "    try:\n",
    "        year = int(val)\n",
    "        # Consider valid if year is between 1900 and 2100 (adjust as needed)\n",
    "        return 1900 <= year <= 2100\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "\n",
    "def get_start_year(row):\n",
    "    status = str(row['Status']).lower()\n",
    "    year_val = row['Start year']\n",
    "    if not is_valid_year(year_val):\n",
    "        if status == 'construction':\n",
    "            return 2028\n",
    "        else:\n",
    "            return 2015\n",
    "    return int(year_val)\n",
    "\n",
    "df_gem_iso['Start year'] = df_gem_iso.apply(get_start_year, axis=1)\n",
    "\n",
    "# Check for missing renewable capacity from IRENA and add to df_gem_iso if needed\n",
    "def add_missing_irena_capacity(df_gem_iso, df_irena_c, input_iso, fuel_type):\n",
    "    \"\"\"\n",
    "    Compare IRENA 2022 renewable capacity with cumulative GEM capacity (years  2022).\n",
    "    If IRENA > GEM, add the difference as a new record for 2022.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Technology mapping for different fuel types\n",
    "    tech_mapping = {\n",
    "        'solar': {'Technology': 'PV', 'Type': 'solar', 'description': 'Solar'},\n",
    "        'wind': {'Technology': 'Onshore', 'Type': 'wind', 'description': 'Wind'},\n",
    "        'hydro': {'Technology': 'conventional storage', 'Type': 'hydropower', 'description': 'Hydro'}\n",
    "    }\n",
    "    \n",
    "    if fuel_type not in tech_mapping:\n",
    "        print(f\"Warning: Unknown fuel type '{fuel_type}'. Skipping.\")\n",
    "        return df_gem_iso\n",
    "    \n",
    "    # Get IRENA capacity for 2022\n",
    "    irena_capacity_2022 = df_irena_c[\n",
    "        (df_irena_c['iso_code'] == input_iso) & \n",
    "        (df_irena_c['model_fuel'] == fuel_type) & \n",
    "        (df_irena_c['Year'] == 2022)\n",
    "    ]['Electricity statistics (MW/GWh)'].sum() / 1000  # Convert MW to GW\n",
    "    \n",
    "    # Get cumulative GEM capacity for years <= 2022\n",
    "    gem_capacity_cumulative = df_gem_iso[\n",
    "        (df_gem_iso['model_fuel'] == fuel_type) & \n",
    "        (df_gem_iso['Start year'] <= 2022)\n",
    "    ]['Capacity (MW)'].sum() / 1000  # Convert MW to GW\n",
    "    \n",
    "    # Calculate the difference\n",
    "    capacity_difference_gw = irena_capacity_2022 - gem_capacity_cumulative\n",
    "    \n",
    "    print(f\"IRENA {fuel_type} capacity 2022: {irena_capacity_2022:.2f} GW\")\n",
    "    print(f\"GEM cumulative {fuel_type} capacity (2022): {gem_capacity_cumulative:.2f} GW\")\n",
    "    print(f\"Difference: {capacity_difference_gw:.2f} GW\")\n",
    "    \n",
    "    # If difference > 0, add missing capacity\n",
    "    if capacity_difference_gw > 0:\n",
    "        print(f\"Adding {capacity_difference_gw:.2f} GW of missing {fuel_type} capacity to df_gem_iso\")\n",
    "        \n",
    "        # Create new row with missing capacity for year 2022\n",
    "        tech_info = tech_mapping[fuel_type]\n",
    "        new_row = pd.Series({\n",
    "            'iso_code': input_iso,\n",
    "            'model_fuel': fuel_type,\n",
    "            'Capacity (MW)': capacity_difference_gw * 1000,  # Convert back to MW\n",
    "            'Plant / Project name': 'Aggregated Plant - IRENA Gap',\n",
    "            'Unit / Phase name': f'Missing {tech_info[\"description\"]} Capacity',\n",
    "            'GEM unit/phase ID': None,\n",
    "            'Status': 'operating',\n",
    "            'Start year': 2022,\n",
    "            'Technology': tech_info['Technology'],\n",
    "            'Type': tech_info['Type']\n",
    "        })\n",
    "        \n",
    "        # Add the new row to df_gem_iso\n",
    "        df_gem_iso = pd.concat([df_gem_iso, new_row.to_frame().T], ignore_index=True)\n",
    "        print(f\"Added new {fuel_type} record for year 2022 with {capacity_difference_gw:.2f} GW\")\n",
    "    else:\n",
    "        print(f\"No missing {fuel_type} capacity to add\")\n",
    "    \n",
    "    return df_gem_iso\n",
    "\n",
    "# Apply the function to add missing IRENA capacity for solar and wind\n",
    "df_gem_iso = add_missing_irena_capacity(df_gem_iso, df_irena_c, input_iso, 'solar')\n",
    "df_gem_iso = add_missing_irena_capacity(df_gem_iso, df_irena_c, input_iso, 'wind')\n",
    "df_gem_iso = add_missing_irena_capacity(df_gem_iso, df_irena_c, input_iso, 'hydro')\n",
    "\n",
    "# Check for missing thermal capacity from EMBER and add to df_gem_iso if needed\n",
    "def add_missing_ember_capacity(df_gem_iso, df_ember, input_iso, fuel_type):\n",
    "    \"\"\"\n",
    "    Compare EMBER 2022 thermal capacity with cumulative GEM capacity (years  2022).\n",
    "    If EMBER > GEM, add the difference as a new record for 2022.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Technology mapping for different fuel types\n",
    "    tech_mapping = {\n",
    "        'bioenergy': {'Technology': 'bioenergy', 'Type': 'bioenergy', 'description': 'Bioenergy'},\n",
    "        'coal': {'Technology': 'subcritical', 'Type': 'coal', 'description': 'Coal'},\n",
    "        'gas': {'Technology': 'combined cycle', 'Type': 'gas', 'description': 'Gas'},\n",
    "        'oil': {'Technology': 'gas turbine', 'Type': 'oil', 'description': 'Oil'}\n",
    "    }\n",
    "    \n",
    "    if fuel_type not in tech_mapping:\n",
    "        print(f\"Warning: Unknown fuel type '{fuel_type}'. Skipping.\")\n",
    "        return df_gem_iso\n",
    "    \n",
    "    # Get EMBER capacity for 2022\n",
    "    ember_capacity_2022 = df_ember[\n",
    "        (df_ember['iso_code'] == input_iso) & \n",
    "        (df_ember['model_fuel'] == fuel_type) & \n",
    "        (df_ember['Year'] == 2022) &\n",
    "        (df_ember['Unit'] == 'GW')\n",
    "    ]['Value'].sum()  # Already in GW\n",
    "    \n",
    "    # Get cumulative GEM capacity for years <= 2022\n",
    "    gem_capacity_cumulative = df_gem_iso[\n",
    "        (df_gem_iso['model_fuel'] == fuel_type) & \n",
    "        (df_gem_iso['Start year'] <= 2022)\n",
    "    ]['Capacity (MW)'].sum() / 1000  # Convert MW to GW\n",
    "    \n",
    "    # Calculate the difference\n",
    "    capacity_difference_gw = ember_capacity_2022 - gem_capacity_cumulative\n",
    "    \n",
    "    print(f\"EMBER {fuel_type} capacity 2022: {ember_capacity_2022:.2f} GW\")\n",
    "    print(f\"GEM cumulative {fuel_type} capacity (2022): {gem_capacity_cumulative:.2f} GW\")\n",
    "    print(f\"Difference: {capacity_difference_gw:.2f} GW\")\n",
    "    \n",
    "    # If difference > 0, add missing capacity\n",
    "    if capacity_difference_gw > 0:\n",
    "        print(f\"Adding {capacity_difference_gw:.2f} GW of missing {fuel_type} capacity to df_gem_iso\")\n",
    "        \n",
    "        # Create new row with missing capacity for year 2022\n",
    "        tech_info = tech_mapping[fuel_type]\n",
    "        new_row = pd.Series({\n",
    "            'iso_code': input_iso,\n",
    "            'model_fuel': fuel_type,\n",
    "            'Capacity (MW)': capacity_difference_gw * 1000,  # Convert back to MW\n",
    "            'Plant / Project name': 'Aggregated Plant - EMBER Gap',\n",
    "            'Unit / Phase name': f'Missing {tech_info[\"description\"]} Capacity',\n",
    "            'GEM unit/phase ID': None,\n",
    "            'Status': 'operating',\n",
    "            'Start year': 2022,\n",
    "            'Technology': tech_info['Technology'],\n",
    "            'Type': tech_info['Type']\n",
    "        })\n",
    "        \n",
    "        # Add the new row to df_gem_iso\n",
    "        df_gem_iso = pd.concat([df_gem_iso, new_row.to_frame().T], ignore_index=True)\n",
    "        print(f\"Added new {fuel_type} record for year 2022 with {capacity_difference_gw:.2f} GW\")\n",
    "    else:\n",
    "        print(f\"No missing {fuel_type} capacity to add\")\n",
    "    \n",
    "    return df_gem_iso\n",
    "\n",
    "# Apply the function to add missing EMBER capacity for bioenergy, coal, and gas\n",
    "df_gem_iso = add_missing_ember_capacity(df_gem_iso, df_ember, input_iso, 'bioenergy')\n",
    "df_gem_iso = add_missing_ember_capacity(df_gem_iso, df_ember, input_iso, 'coal')\n",
    "df_gem_iso = add_missing_ember_capacity(df_gem_iso, df_ember, input_iso, 'gas')\n",
    "\n",
    "\n",
    "# Join df_gem_iso with df_gem_map on 'model_fuel' and 'Technology'\n",
    "# Using left join to preserve gap-filling records that might not have exact mappings\n",
    "df_gem_iso = df_gem_iso.merge(df_gem_map[['model_fuel', 'Technology', 'model_name']], left_on=['model_fuel', 'Technology'], right_on=['model_fuel', 'Technology'], how='left')\n",
    "\n",
    "# Fill missing model_names for gap-filling records\n",
    "df_gem_iso['model_name'] = df_gem_iso['model_name'].fillna(df_gem_iso['model_fuel'].apply(lambda x: f'ep_{x}'))\n",
    "\n",
    "# add capacity factor and grid cell to df_gem_iso\n",
    "df_gem_iso = df_gem_iso.merge(re_units_cf_grid_cell_mapping,\n",
    "    left_on=['GEM unit/phase ID'], \n",
    "    right_on=['GEM_unit/phase_ID'], \n",
    "    how='left')\n",
    "\n",
    "\n",
    "df_gem_iso.to_excel(\"output/df_gem_iso_after_gap_filling.xlsx\", index=False)\n",
    "\n",
    "\n",
    "# df_gem_iso.to_excel(\"output/df_gem_iso.xlsx\", index=False)\n",
    "\n",
    "\n",
    "def apply_get_costs_and_eff(row):\n",
    "    iso = row['iso_code']\n",
    "    size = row['Capacity (MW)'] if 'Capacity (MW)' in row else None\n",
    "    model_name = row['model_name'].lower() if 'model_name' in row else None\n",
    "    year = row['Start year'] if 'Start year' in row else None\n",
    "    if None in (iso, size, model_name, year):\n",
    "        return [None, None, None, None]\n",
    "    try:\n",
    "        res = get_costs_and_eff(iso, size, model_name, year)\n",
    "        return res.iloc[0].tolist()\n",
    "    except Exception as e:\n",
    "        return [None, None, None, None]\n",
    "\n",
    "# Define the names of the new columns\n",
    "new_cols = ['capex', 'fixom', 'varom', 'efficiency']\n",
    "\n",
    "df_gem_iso[new_cols] = df_gem_iso.apply(apply_get_costs_and_eff, axis=1, result_type='expand')\n",
    "\n",
    "# Interactive DuckDB query to find records with null Start year\n",
    "# query = duckdb.sql(\"SELECT * FROM df_gem_iso WHERE \\\"Start year\\\" IS NULL\").df()\n",
    "# display(query)\n",
    "\n",
    "# Calibrate efficiency for gas and coal to match UNSD fuel consumption\n",
    "df_gem_iso.loc[df_gem_iso['model_name'].str.lower().str.startswith('ep_gas'), 'efficiency'] = (\n",
    "    df_gem_iso.loc[df_gem_iso['model_name'].str.lower().str.startswith('ep_gas'), 'efficiency'] * efficiency_adjustment_gas\n",
    ")\n",
    "\n",
    "df_gem_iso.loc[df_gem_iso['model_name'].str.lower().str.startswith('ep_coal'), 'efficiency'] = (\n",
    "    df_gem_iso.loc[df_gem_iso['model_name'].str.lower().str.startswith('ep_coal'), 'efficiency'] * efficiency_adjustment_coal\n",
    ")\n",
    "\n",
    "\n",
    "#############################################################################################################################\n",
    "# need to look into why ep_solar_PV is turning up with no efficiency. forcing an update here\n",
    "df_gem_iso.loc[(df_gem_iso['efficiency'].isnull()) | (df_gem_iso['efficiency'] == 0), 'efficiency'] = 0.33123\n",
    "#############################################################################################################################\n",
    "\n",
    "query = f\"\"\"\n",
    "    SELECT  \n",
    "        (\n",
    "            CASE \n",
    "                WHEN \"Capacity (MW)\" >= {capacity_threshold} THEN CAST(model_name AS VARCHAR) || '_' || COALESCE(CAST(\"GEM unit/phase ID\" AS VARCHAR), '')\n",
    "                ELSE CAST(model_name AS VARCHAR)\n",
    "            END\n",
    "            ||\n",
    "            CASE \n",
    "                WHEN lower(Status) NOT IN ('operating', 'construction') THEN '__m'\n",
    "                ELSE ''\n",
    "            END\n",
    "        ) AS model_name,\n",
    "        CASE\n",
    "            WHEN \"Capacity (MW)\" >= {capacity_threshold} THEN CAST(\"Plant / Project name\" AS VARCHAR) || '_' || COALESCE(CAST(\"Unit / Phase name\" AS VARCHAR), '')\n",
    "            ELSE 'Aggregated Plant'\n",
    "        END AS model_description,\n",
    "        model_fuel,\n",
    "        iso_code, \n",
    "        \"Start year\",\n",
    "        CASE \n",
    "            WHEN \"Capacity (MW)\" >= {capacity_threshold} THEN coalesce(CAST(\"Retired year\" AS VARCHAR), '')\n",
    "            ELSE ''\n",
    "        END AS \"retirement_year\",\n",
    "        CASE \n",
    "            WHEN \"Capacity (MW)\" >= {capacity_threshold} THEN coalesce(CAST(\"Subnational unit (state, province)\" AS VARCHAR), '')\n",
    "            ELSE ''\n",
    "        END AS \"state\",\n",
    "        CASE \n",
    "            WHEN \"Capacity (MW)\" >= {capacity_threshold} THEN coalesce(CAST(\"City\" AS VARCHAR), '')\n",
    "            ELSE ''\n",
    "        END AS \"city\",        \n",
    "        CASE \n",
    "            WHEN lower(Status) IN ('operating', 'construction') THEN 'active'\n",
    "            ELSE 'mothballed'\n",
    "        END AS status_group,\n",
    "        SUM(\"Capacity (MW)\") / 1000 AS Capacity_GW,\n",
    "        SUM(\"Capacity (MW)\" * capex) / NULLIF(SUM(\"Capacity (MW)\"), 0) AS capex,\n",
    "        SUM(\"Capacity (MW)\" * fixom) / NULLIF(SUM(\"Capacity (MW)\"), 0) AS fixom,\n",
    "        SUM(\"Capacity (MW)\" * varom) / NULLIF(SUM(\"Capacity (MW)\"), 0) AS varom,\n",
    "        SUM(\"Capacity (MW)\" * efficiency) / NULLIF(SUM(\"Capacity (MW)\"), 0) AS efficiency\n",
    "    FROM df_gem_iso\n",
    "    GROUP BY iso_code, \"Start year\", model_fuel,\n",
    "        CASE \n",
    "            WHEN \"Capacity (MW)\" >= {capacity_threshold} THEN coalesce(CAST(\"Retired year\" AS VARCHAR), '')\n",
    "            ELSE ''\n",
    "        END,\n",
    "        CASE \n",
    "            WHEN lower(Status) IN ('operating', 'construction') THEN 'active'\n",
    "            ELSE 'mothballed'\n",
    "        END,\n",
    "        (\n",
    "            CASE \n",
    "                WHEN \"Capacity (MW)\" >= {capacity_threshold} THEN CAST(model_name AS VARCHAR) || '_' || COALESCE(CAST(\"GEM unit/phase ID\" AS VARCHAR), '')\n",
    "                ELSE CAST(model_name AS VARCHAR)\n",
    "            END\n",
    "            ||\n",
    "            CASE \n",
    "                WHEN lower(Status) NOT IN ('operating', 'construction') THEN '__m'\n",
    "                ELSE ''\n",
    "            END\n",
    "        ),\n",
    "        CASE\n",
    "            WHEN \"Capacity (MW)\" >= {capacity_threshold} THEN CAST(\"Plant / Project name\" AS VARCHAR) || '_' || COALESCE(CAST(\"Unit / Phase name\" AS VARCHAR), '')\n",
    "            ELSE 'Aggregated Plant'\n",
    "        END,\n",
    "        CASE \n",
    "            WHEN \"Capacity (MW)\" >= {capacity_threshold} THEN coalesce(CAST(\"Subnational unit (state, province)\" AS VARCHAR), '')\n",
    "            ELSE ''\n",
    "        END,\n",
    "        CASE \n",
    "            WHEN \"Capacity (MW)\" >= {capacity_threshold} THEN coalesce(CAST(\"City\" AS VARCHAR), '')\n",
    "            ELSE ''\n",
    "        END\n",
    "\"\"\"\n",
    "\n",
    "df_grouped_gem = duckdb.sql(query).df()\n",
    "\n",
    "# add UC parameters to df_grouped_gem\n",
    "\n",
    "def load_uc_data():\n",
    "    \"\"\"Load unit commitment data and mappings\"\"\"\n",
    "    uc_data = pd.read_excel('data/technologies/advanced_parameters.xlsx', sheet_name='uc_data')\n",
    "    uc_tech_map = pd.read_excel('data/technologies/advanced_parameters.xlsx', sheet_name='uc_tech_map')\n",
    "    return uc_data, uc_tech_map\n",
    "\n",
    "def find_technology_for_model(model_name, uc_tech_map):\n",
    "    \"\"\"Find technology using simple startswith matching\"\"\"\n",
    "    for _, row in uc_tech_map.iterrows():\n",
    "        technology = row['technology']\n",
    "        model_patterns = row['model_name'].split(',')\n",
    "        \n",
    "        for pattern in model_patterns:\n",
    "            pattern = pattern.strip()\n",
    "            if model_name.startswith(pattern):\n",
    "                return technology\n",
    "    \n",
    "    return None\n",
    "\n",
    "def determine_size_class(technology, capacity_mw):\n",
    "    \"\"\"Determine size class based on technology and capacity in MW\"\"\"\n",
    "    size_rules = {\n",
    "        'OCGT (Peaker)': [(50, '<50 MW'), (200, '50-200 MW'), (float('inf'), '>200 MW')],\n",
    "        'CCGT': [(300, '<300 MW'), (float('inf'), '>300 MW')],\n",
    "        'Gas/Oil Steam': [(200, '<200 MW'), (float('inf'), '>200 MW')],\n",
    "        'Diesel': [(20, '<20 MW'), (float('inf'), '>20 MW')],\n",
    "        'Subcritical Coal': [(300, '<300 MW'), (float('inf'), '>300 MW')],\n",
    "        'Supercritical Coal': [(500, '<500 MW'), (float('inf'), '>500 MW')],\n",
    "        'Nuclear': [(float('inf'), 'All')]\n",
    "    }\n",
    "    \n",
    "    if technology in size_rules:\n",
    "        for threshold, size_class in size_rules[technology]:\n",
    "            if capacity_mw < threshold:\n",
    "                return size_class\n",
    "    return None\n",
    "\n",
    "def get_uc_parameters(uc_data, technology, size_class):\n",
    "    \"\"\"Get unit commitment parameters for given technology and size class\"\"\"\n",
    "    mask = (uc_data['technology'] == technology) & (uc_data['Size Class'] == size_class)\n",
    "    matching_rows = uc_data[mask]\n",
    "    \n",
    "    if len(matching_rows) == 0:\n",
    "        return None\n",
    "    \n",
    "    row = matching_rows.iloc[0]\n",
    "    return {\n",
    "        'min_stable_factor_pct': row['Min Stable Factor (%)'],\n",
    "        'min_up_time_h': row['Min Up Time (h)'],\n",
    "        'min_down_time_h': row['Min Down Time (h)'],\n",
    "        'max_ramp_up_pct_h': row['Max Ramp Up (%/h)'],\n",
    "        'max_ramp_down_pct_h': row['Max Ramp Down (%/h)'],\n",
    "        'startup_time_h': row['Startup Time (h)'],\n",
    "        'startup_cost_per_mw': row['Startup Cost ($/MW)'],\n",
    "        'shutdown_cost_per_mw': row['Shutdown Cost ($/MW)']\n",
    "    }\n",
    "\n",
    "def add_uc_parameters_to_df(df, uc_data, uc_tech_map):\n",
    "    \"\"\"Add unit commitment parameters using simple startswith matching\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Initialize new columns\n",
    "    df['uc_technology'] = ''\n",
    "    df['uc_size_class'] = ''\n",
    "    \n",
    "    for col in ['min_stable_factor_pct', 'min_up_time_h', 'min_down_time_h', \n",
    "                'max_ramp_up_pct_h', 'max_ramp_down_pct_h', 'startup_time_h', \n",
    "                'startup_cost_per_mw', 'shutdown_cost_per_mw']:\n",
    "        df[col] = np.nan\n",
    "    \n",
    "    # Track stats\n",
    "    matched_count = 0\n",
    "    total_thermal = 0\n",
    "    \n",
    "    # Process each plant\n",
    "    for idx, row in df.iterrows():\n",
    "        model_name = row['model_name']\n",
    "        capacity_mw = row['Capacity_GW'] * 1000  # Convert GW to MW\n",
    "        \n",
    "        # Simple startswith matching - much cleaner!\n",
    "        technology = find_technology_for_model(model_name, uc_tech_map)\n",
    "        \n",
    "        if technology is None:\n",
    "            continue  # Skip non-thermal plants\n",
    "        \n",
    "        total_thermal += 1\n",
    "        size_class = determine_size_class(technology, capacity_mw)\n",
    "        \n",
    "        if size_class:\n",
    "            uc_params = get_uc_parameters(uc_data, technology, size_class)\n",
    "            if uc_params:\n",
    "                df.at[idx, 'uc_technology'] = technology\n",
    "                df.at[idx, 'uc_size_class'] = size_class\n",
    "                \n",
    "                for param_name, param_value in uc_params.items():\n",
    "                    df.at[idx, param_name] = param_value\n",
    "                \n",
    "                matched_count += 1\n",
    "    \n",
    "    # print(f\" UC Results: {matched_count}/{total_thermal} thermal plants matched ({matched_count/total_thermal*100:.1f}%)\")\n",
    "    return df\n",
    "\n",
    "# Apply to your Japanese dataframe:\n",
    "uc_data, uc_tech_map = load_uc_data()\n",
    "df_grouped_gem_with_uc = add_uc_parameters_to_df(df_grouped_gem, uc_data, uc_tech_map)\n",
    "\n",
    "df_grouped_gem_with_uc = df_grouped_gem_with_uc.drop(columns=['uc_technology', 'uc_size_class'])\n",
    "\n",
    "\n",
    "output_path = f\"output/VerveStacks_{input_iso}.xlsx\"\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    # Using xlwings to write existing stock data\n",
    "    app = xw.App(visible=False)\n",
    "    try:\n",
    "        wb = app.books.open(output_path)\n",
    "        \n",
    "        # Create or replace the sheet\n",
    "        if 'existing_stock' in [ws.name for ws in wb.sheets]:\n",
    "            wb.sheets['existing_stock'].delete()\n",
    "        \n",
    "        ws = wb.sheets.add('existing_stock')\n",
    "        \n",
    "        # Write the data\n",
    "        df_sorted = df_grouped_gem_with_uc.sort_values(by=['model_name', 'Start year'])\n",
    "        ws.range('A1').value = [df_sorted.columns.tolist()] + df_sorted.values.tolist()\n",
    "        \n",
    "        wb.save()\n",
    "        wb.close()\n",
    "    finally:\n",
    "        app.quit()\n",
    "\n",
    "\n",
    "# function calls\n",
    "#############################################################################################################################\n",
    "\n",
    "\n",
    "calibration_data(input_iso,df_irena_util,df_ember_util,df_grouped_gem)\n",
    "ccs_retrofits(input_iso,df_grouped_gem)\n",
    "re_targets_ember(input_iso)\n",
    "get_weo_data(input_iso)\n",
    "get_iamc_data(input_iso)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
